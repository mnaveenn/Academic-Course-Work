---
title: "Assignment 5 Clustering"
author: "Naveen Narayanan Meyyappan"
date: "07/11/2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Kmeans Clustering

Kmeans clustering is a type of unsupervised learning 

```{r, include=FALSE}
library(ggplot2)
```

Let us read the data frame

```{r}
data = read.csv("kmeans.csv")
summary(data)
```

The data has 5 numeric columns a,b,c,d and e and 446 records.

Now we have to generate 3 clusters from this data using the kmeans algorithm.
Let us define the kclus function which performs the clustering of data.

```{r}
set.seed(100) #Set the seed so that the same set of random numbers are generated and the result doesnot change

# Kmeans function
kclus <- function(data, nclus) 
        {
        # Generating 3 random cluster centers for each column in the data  
        acen <- data[1:nclus,1]
        bcen <- data[1:nclus,2]
        ccen <- data[1:nclus,3]
        dcen <- data[1:nclus,4]
        ecen <- data[1:nclus,5]
        #Adding a column to the data frame and renaming the columns
        data <- data.frame(aval = data[,1], bval = data[,2], cval = data[,3], dval = data[,4], eval = data[,5], clus = NA)
        #Creating a new dataframe called cluster which maintains the centers of each cluster
        clus <- data.frame(name = 1:nclus, acen = acen, bcen = bcen, ccen = ccen, dcen = dcen, ecen = ecen)
        
        # Defining a flag which determines when to stop the algorithm
        finish <- FALSE
        
        # Here l stores the dimensions of the data
        l<-dim(data)
        
        while(finish == FALSE) 
                {
                for(i in 1:l[1]) 
                        {
                        # For each data point calculate the distances between the centers and the datapoints
                        dist <- sqrt((data[i,1]-clus$acen)^2 + (data[i,2]-clus$bcen)^2 + (data[i,3]-clus$ccen)^2 + (data[i,4]-clus$dcen)^2 + (data[i,5]-clus$ecen)^2)
                        # Now assign the data point to one of the nclus clusters.
                        data$clus[i] <- which.min(dist) #The data point is assigned to the cluster for which the distance is minimum among the nclus clusters
                        }
                #Store the privous centers in a separate column vector
                acen_old <- clus$acen
                bcen_old <- clus$bcen
                ccen_old <- clus$ccen
                dcen_old <- clus$dcen
                ecen_old <- clus$ecen
                
                for(i in 1:nclus) 
                        {
                        # Calculate the new mean of the clusters and update the cluster data frame
                        clus[i,2] <- mean(subset(data$aval, data$clus == i))
                        clus[i,3] <- mean(subset(data$bval, data$clus == i))
                        clus[i,4] <- mean(subset(data$cval, data$clus == i))
                        clus[i,5] <- mean(subset(data$dval, data$clus == i))
                        clus[i,6] <- mean(subset(data$eval, data$clus == i))
                        }
                # We can stop this process when there is no more change in the cluster centers
                if(identical(acen_old, clus$acen) & identical(bcen_old, clus$bcen) & identical(ccen_old, clus$ccen) & identical(dcen_old, clus$dcen) & identical(ecen_old, clus$ecen))
                        {
                        finish <- TRUE # End the loop by changing the flag
                        }
        }
        output<-NULL
        output$data<-data
        output$cluster<-clus
        return(output) #Returning both the data and cluster data frames 
}
```

Let us call the kclus function on our data set

```{r}
output <- kclus(data, 3) 
# The above ouput variable has both the data and cluster centers

#Let us have a look at the output data frame
head(output$data) # Output is a data frame with an additional clus column. This column indicates the cluster to which the data point belongs 
table(output$data[,6]) # The output below indicates the number of data point that belongs to the various clusters

# Now let us look at the covariance of the cluster matrix
print(cov(output$cluster))
```

The output is plotted as shown below:

The following is a plot between the 2 columns from the data ("a" and "b"). The clusters are plotted with different colors
```{r}
ggplot(output$data, aes(bval, aval, color = as.factor(clus))) + geom_point()
```

Similarly, we can call other plots by changing the values on the 2 axes. Another plot would be between columns "d" and "e"

```{r}
ggplot(output$data, aes(eval, dval, color = as.factor(clus))) + geom_point()
```

Now we can use the inbuilt pairs plot to get a view of all the plots between the different columns

```{r fig3, fig.height = 9, fig.width = 9, fig.align = "center"}
X <- output$data[,1:5]
y <- output$data[,6]
y_col <- c('#F8766D', '#00BA38', '#619CFF')

pairs(X, col = y_col[y])
```


### References
1. <https://en.wikipedia.org/wiki/K-means_clustering>
2. <https://stackoverflow.com/questions/41912875/writing-own-kmeans-algorithm-in-r>














